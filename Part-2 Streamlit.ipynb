{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408b215d",
   "metadata": {},
   "source": [
    "# ENCODAGES ET SÉPARATION TRAIN/TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0dae142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: module://matplotlib_inline.backend_inline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53beffad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4873455 entries, 0 to 4873454\n",
      "Data columns (total 37 columns):\n",
      " #   Column                       Dtype  \n",
      "---  ------                       -----  \n",
      " 0   typedebien                   object \n",
      " 1   typedetransaction            object \n",
      " 2   etage                        int64  \n",
      " 3   surface                      int64  \n",
      " 4   surface_terrain              float64\n",
      " 5   nb_pieces                    int64  \n",
      " 6   balcon                       int64  \n",
      " 7   eau                          int64  \n",
      " 8   bain                         int64  \n",
      " 9   dpeL                         object \n",
      " 10  dpeC                         float64\n",
      " 11  mapCoordonneesLatitude       float64\n",
      " 12  mapCoordonneesLongitude      float64\n",
      " 13  nb_etages                    float64\n",
      " 14  places_parking               float64\n",
      " 15  cave                         bool   \n",
      " 16  exposition                   object \n",
      " 17  ges_class                    object \n",
      " 18  annee_construction           object \n",
      " 19  nb_toilettes                 float64\n",
      " 20  porte_digicode               bool   \n",
      " 21  ascenseur                    bool   \n",
      " 22  charges_copro                float64\n",
      " 23  chauffage_energie            object \n",
      " 24  chauffage_systeme            object \n",
      " 25  chauffage_mode               object \n",
      " 26  logement_neuf                object \n",
      " 27  date                         object \n",
      " 28  INSEE_COM                    object \n",
      " 29  loyer_m2_median_n6           float64\n",
      " 30  nb_log_n6                    float64\n",
      " 31  taux_rendement_n6            float64\n",
      " 32  loyer_m2_median_n7           float64\n",
      " 33  nb_log_n7                    float64\n",
      " 34  taux_rendement_n7            float64\n",
      " 35  prix_m2_vente                float64\n",
      " 36  chauffage_energie_principal  object \n",
      "dtypes: bool(3), float64(15), int64(6), object(13)\n",
      "memory usage: 1.2+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## paths\n",
    "folder_path_M = '/Users/maximehenon/Documents/GitHub/MAR25_BDS_Compagnon_Immo/'\n",
    "# folder_path_Y = 'C:/Users/charl/OneDrive/Documents/Yasmine/DATASCIENTEST/FEV25-BDS-COMPAGNON'\n",
    "#folder_path_C = '../data/processed/Sales'\n",
    "# folder_path_L= '/Users/loick.d/Documents/Datascientest/Github immo/MAR25_BDS_Compagnon_Immo/'\n",
    "# folder_path_LW = 'C:/Users/User/Downloads/drive-download-20250508T155351Z-1-001'\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "output_file = os.path.join(folder_path_M, 'df_sales_clean.csv')\n",
    "# output_file = os.path.join(folder_path_Y, 'df_sales_clean.csv')\n",
    "#output_file = os.path.join(folder_path_C, 'df_sales_clean.csv')\n",
    "#output_file = os.path.join(folder_path_L, 'df_sales_clean.csv')\n",
    "# output_file = os.path.join(folder_path_LW, 'df_sales_clean.csv')\n",
    "\n",
    "chunksize = 100000  # Number of rows per chunk\n",
    "chunks = pd.read_csv(output_file, sep=';', chunksize=chunksize, index_col=None, on_bad_lines='skip', low_memory=False)\n",
    "# Process chunks\n",
    "df_sales_clean = pd.concat(chunk for chunk in chunks)\n",
    "\n",
    "## Rappel des colonnes restantes\n",
    "# print(\"Colonnes restantes dans le DataFrame :\")\n",
    "# print(df_sales_clean.columns)\n",
    "# print(df_sales_clean.dtypes)\n",
    "# print(\"\\nShape du Dataset après élimination des colonnes :\", df_sales_clean.shape)\n",
    "print(df_sales_clean.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c5165b",
   "metadata": {},
   "source": [
    "## Encodages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407fa52",
   "metadata": {},
   "source": [
    "### Catégorielles ordinales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f0eac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes encodées :\n",
      "Index(['ges_class', 'dpeL', 'logement_neuf', 'nb_pieces', 'bain', 'eau',\n",
      "       'nb_toilettes', 'balcon'],\n",
      "      dtype='object')\n",
      "\n",
      "Dtypes des colonnes encodées :\n",
      "ges_class        float64\n",
      "dpeL             float64\n",
      "logement_neuf    float64\n",
      "nb_pieces        float64\n",
      "bain             float64\n",
      "eau              float64\n",
      "nb_toilettes     float64\n",
      "balcon           float64\n",
      "dtype: object\n",
      "\n",
      "Shape du DataFrame après encodage des colonnes ordinales : (4873455, 37)\n"
     ]
    }
   ],
   "source": [
    "# Categorielles Ordinales    #\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Liste des colonnes ordinales\n",
    "ordinales = [\"ges_class\", \"dpeL\", \"logement_neuf\", \"nb_pieces\", \"bain\", \"eau\", \"nb_toilettes\", \"balcon\"]\n",
    "\n",
    "# Création d'une pipeline pour l'imputation et l'encodage\n",
    "pipeline_ordinale = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encode\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n",
    "])\n",
    "\n",
    "# Application de la pipeline sur les colonnes ordinales\n",
    "ord_transformed = pipeline_ordinale.fit_transform(df_sales_clean[ordinales])\n",
    "\n",
    "# Conversion du résultat en DataFrame\n",
    "ord_columns = ordinales\n",
    "ord_encoded = pd.DataFrame(ord_transformed, columns=ord_columns, index=df_sales_clean.index)\n",
    "\n",
    "# Remplacement des colonnes originales par les colonnes encodées\n",
    "df_sales_clean = df_sales_clean.drop(columns=ordinales).join(ord_encoded)\n",
    "\n",
    "# Affichage des colonnes encodées\n",
    "print(\"Colonnes encodées :\")\n",
    "print(ord_encoded.columns)\n",
    "print(\"\\nDtypes des colonnes encodées :\")\n",
    "print(ord_encoded.dtypes)\n",
    "print(\"\\nShape du DataFrame après encodage des colonnes ordinales :\", df_sales_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ca89a",
   "metadata": {},
   "source": [
    "### Catégorielles non-ordinales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5759c410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes avec des valeurs manquantes :\n",
      "Index(['surface_terrain', 'dpeC', 'places_parking', 'exposition',\n",
      "       'annee_construction', 'charges_copro', 'chauffage_energie',\n",
      "       'chauffage_systeme', 'chauffage_mode', 'loyer_m2_median_n6',\n",
      "       'nb_log_n6', 'taux_rendement_n6', 'loyer_m2_median_n7', 'nb_log_n7',\n",
      "       'taux_rendement_n7', 'chauffage_energie_principal'],\n",
      "      dtype='object')\n",
      "Colonnes pseudo-catégoriques avec des valeurs manquantes :\n",
      "[]\n",
      "Colonnes textuelles avec des valeurs manquantes :\n",
      "['exposition', 'annee_construction', 'chauffage_energie', 'chauffage_systeme', 'chauffage_mode', 'chauffage_energie_principal']\n",
      "Valeurs manquantes après imputation :\n",
      "surface_terrain                2439719\n",
      "dpeC                           1692044\n",
      "places_parking                 3073570\n",
      "exposition                           0\n",
      "annee_construction                   0\n",
      "charges_copro                  3562782\n",
      "chauffage_energie                    0\n",
      "chauffage_systeme                    0\n",
      "chauffage_mode                       0\n",
      "loyer_m2_median_n6             1439167\n",
      "nb_log_n6                      1439167\n",
      "taux_rendement_n6              1439167\n",
      "loyer_m2_median_n7             1081538\n",
      "nb_log_n7                      1081538\n",
      "taux_rendement_n7              1081538\n",
      "chauffage_energie_principal          0\n",
      "dtype: int64\n",
      "Colonnes encodées en target :\n",
      "['etage', 'nb_etages', 'exposition', 'annee_construction', 'chauffage_energie', 'chauffage_systeme', 'date']\n",
      "\n",
      "Colonnes encodées en one-hot :\n",
      "\n",
      "['typedebien', 'typedetransaction', 'chauffage_mode', 'chauffage_energie_principal']\n",
      "\n",
      "Colonnes One-Hot initiales supprimées : ['typedebien', 'typedetransaction', 'chauffage_mode', 'chauffage_energie_principal']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# colonne de regroupement\n",
    "GROUP_COL    = 'INSEE_COM'  \n",
    "\n",
    "# Identification des colonnes pseudo-catégoriques\n",
    "pseudo_categorical_ints = [col for col in ['etage', 'nb_etages'] if col in df_sales_clean.columns] \n",
    "# Conversion en string\n",
    "df_sales_clean[pseudo_categorical_ints] = df_sales_clean[pseudo_categorical_ints].astype(str)\n",
    "\n",
    "\n",
    "# ---------- Étape 1 : Gestion des valeurs manquantes ----------\n",
    "# (Imputation des colonnes pseudo-catégoriques et textuelles)\n",
    "\n",
    "# Identifier les colonnes avec des valeurs manquantes\n",
    "missing_cols = df_sales_clean.columns[df_sales_clean.isnull().any()]\n",
    "print(\"Colonnes avec des valeurs manquantes :\")\n",
    "print(missing_cols)\n",
    "\n",
    "# Colonnes pseudo-catégoriques avec des valeurs manquantes\n",
    "missing_pseudo_categorical = [col for col in pseudo_categorical_ints if col in missing_cols]\n",
    "print(\"Colonnes pseudo-catégoriques avec des valeurs manquantes :\")\n",
    "print(missing_pseudo_categorical)\n",
    "\n",
    "# Colonnes textuelles avec des valeurs manquantes\n",
    "missing_text_cols = [col for col in df_sales_clean.select_dtypes(include=['object']).columns if col in missing_cols]\n",
    "print(\"Colonnes textuelles avec des valeurs manquantes :\")\n",
    "print(missing_text_cols)\n",
    "\n",
    "# Imputation des colonnes pseudo-catégoriques par la médiane par INSEE_COM\n",
    "for col in missing_pseudo_categorical:\n",
    "    group_medians = df_sales_clean.groupby(GROUP_COL)[col].median()\n",
    "    global_median = df_sales_clean[col].median()\n",
    "    df_sales_clean[col] = df_sales_clean[GROUP_COL].map(group_medians).fillna(global_median)\n",
    "\n",
    "# Imputation des colonnes textuelles avec une valeur par défaut\n",
    "for col in missing_text_cols:\n",
    "    df_sales_clean[col] = df_sales_clean[col].fillna(\"MISSING\")\n",
    "\n",
    "# Vérification\n",
    "print(\"Valeurs manquantes après imputation :\")\n",
    "print(df_sales_clean[missing_cols].isnull().sum())\n",
    "\n",
    "\n",
    "# ---------- Étape 2: lister les colonnes catégoriques ----------\n",
    "\n",
    "# Conversion des colonnes pseudo-catégoriques en string pour les traiter comme catégoriques\n",
    "df_sales_clean[pseudo_categorical_ints] = df_sales_clean[pseudo_categorical_ints].astype(str)\n",
    "cat_cols = [col for col in pseudo_categorical_ints]\n",
    "\n",
    "# Ajout des colonnes de type object\n",
    "object_cols = df_sales_clean.select_dtypes(include=['object']).columns\n",
    "cat_cols.extend([col for col in object_cols if col not in cat_cols])\n",
    "\n",
    "# Exclusions\n",
    "cat_cols = [col for col in cat_cols if col not in ordinales]\n",
    "cat_cols = [col for col in cat_cols if df_sales_clean[col].dtype != 'bool']\n",
    "cat_cols = [col for col in cat_cols if col != GROUP_COL]\n",
    "cat_cols = [col for col in cat_cols if col in df_sales_clean.columns]\n",
    "\n",
    "\n",
    "# ---------- Etape 2: séparer les colonnes catégoriques en fonction de leur cardinalité ----------\n",
    "cat_cols_large = [col for col in cat_cols if df_sales_clean[col].nunique() > 10]\n",
    "cat_cols_small = [col for col in cat_cols if df_sales_clean[col].nunique() <= 10]\n",
    "\n",
    "\n",
    "# ---------- Etape 3: Appliquer un Target-Encoding pour les colonnes à forte cardinalité  ----------\n",
    "\n",
    "if cat_cols_large:\n",
    "    df_sales_clean[cat_cols_large] = df_sales_clean[cat_cols_large].fillna(\"MISSING\")\n",
    "    target_encoder = TargetEncoder(cols=cat_cols_large)\n",
    "    encoded_large = target_encoder.fit_transform(df_sales_clean[cat_cols_large], df_sales_clean[\"prix_m2_vente\"])\n",
    "    df_sales_clean.drop(columns=cat_cols_large, inplace=True)\n",
    "    df_sales_clean = df_sales_clean.join(\n",
    "        pd.DataFrame(encoded_large, columns=cat_cols_large, index=df_sales_clean.index))\n",
    "else:\n",
    "    print(\"Aucune colonne à plus de 10 modalités.\")\n",
    "\n",
    "\n",
    "# ---------- Etape 4: Appliquer un One_hot encoding pour les coloones à faible cardinalité ----------\n",
    "if cat_cols_small:\n",
    "    one_hot_pipeline = Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"MISSING\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "    ])\n",
    "    ohe_encoded = one_hot_pipeline.fit_transform(df_sales_clean[cat_cols_small])\n",
    "    ohe_col_names = one_hot_pipeline.named_steps[\"onehot\"].get_feature_names_out(cat_cols_small)\n",
    "    df_sales_clean.drop(columns=cat_cols_small, inplace=True)\n",
    "    df_sales_clean = df_sales_clean.join(\n",
    "        pd.DataFrame(ohe_encoded, columns=ohe_col_names, index=df_sales_clean.index))\n",
    "else:\n",
    "    print(\"Aucune colonne à 10 modalités ou moins.\")\n",
    "\n",
    "# ---------- Etape 5: Vérification des colonnes encodées ----------\n",
    "# Affichage des colonnes encodées en target et en one-hot\n",
    "print(\"Colonnes encodées en target :\")\n",
    "print(cat_cols_large)\n",
    "print(\"\\nColonnes encodées en one-hot :\\n\")\n",
    "print(cat_cols_small)\n",
    "\n",
    "# ----------- Etape 6 : supprimer les colonnes initiales après encodage ----------\n",
    "\n",
    "df_sales_clean.drop(columns=[col for col in cat_cols_small if col in df_sales_clean.columns], inplace=True)\n",
    "\n",
    "# Vérification après suppression\n",
    "print(\"\\nColonnes One-Hot initiales supprimées :\", [col for col in cat_cols_small if col not in df_sales_clean.columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989ad875",
   "metadata": {},
   "source": [
    "### Variables géographiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daaad551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes géographiques :\n",
      "['x_geo', 'y_geo', 'z_geo']\n",
      "\n",
      "Dtypes des colonnes géographiques :\n",
      "x_geo    float64\n",
      "y_geo    float64\n",
      "z_geo    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Conversion des coordonnées géographiques en radians\n",
    "lat_rad = np.radians(df_sales_clean['mapCoordonneesLatitude'].values)\n",
    "lon_rad = np.radians(df_sales_clean['mapCoordonneesLongitude'].values)\n",
    "\n",
    "# Projection sur la sphère unité\n",
    "df_sales_clean['x_geo'] = np.cos(lat_rad) * np.cos(lon_rad)\n",
    "df_sales_clean['y_geo'] = np.cos(lat_rad) * np.sin(lon_rad)\n",
    "df_sales_clean['z_geo'] = np.sin(lat_rad)\n",
    "\n",
    "# Suppression des colonnes Latitude et Longitude\n",
    "df_sales_clean = df_sales_clean.drop(columns=['mapCoordonneesLongitude', 'mapCoordonneesLatitude'])\n",
    "\n",
    "# Affichage des colonnes encodées\n",
    "print(\"Colonnes géographiques :\")\n",
    "print(['x_geo', 'y_geo', 'z_geo'])\n",
    "print(\"\\nDtypes des colonnes géographiques :\")\n",
    "print(df_sales_clean[['x_geo', 'y_geo', 'z_geo']].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565bafbf",
   "metadata": {},
   "source": [
    "### Variables numériques continues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cefbe63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes numériques traitées :\n",
      "['surface', 'surface_terrain', 'dpeC', 'places_parking', 'charges_copro', 'loyer_m2_median_n6', 'nb_log_n6', 'taux_rendement_n6', 'loyer_m2_median_n7', 'nb_log_n7', 'taux_rendement_n7', 'prix_m2_vente']\n",
      "\n",
      "Valeurs manquantes après imputation :\n",
      "surface               0\n",
      "surface_terrain       0\n",
      "dpeC                  0\n",
      "places_parking        0\n",
      "charges_copro         0\n",
      "loyer_m2_median_n6    0\n",
      "nb_log_n6             0\n",
      "taux_rendement_n6     0\n",
      "loyer_m2_median_n7    0\n",
      "nb_log_n7             0\n",
      "taux_rendement_n7     0\n",
      "prix_m2_vente         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "GROUP_COL = 'INSEE_COM'  # Colonne de regroupement\n",
    "\n",
    "# ---------- Étape 1 : Imputation des valeurs manquantes pour les colonnes numériques ----------\n",
    "\n",
    "# Sélection des colonnes numériques\n",
    "numeric_cols = df_sales_clean.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Exclusion des colonnes ordinales, pseudo-catégoriques, catégoriques (small et large), géographiques, et One-Hot encodées\n",
    "numeric_cols = [\n",
    "    col for col in numeric_cols\n",
    "    if col not in (ordinales + pseudo_categorical_ints + cat_cols_small + cat_cols_large + list(ohe_col_names) + ['x_geo', 'y_geo', 'z_geo'])\n",
    "]\n",
    "\n",
    "# Imputation des valeurs manquantes par la médiane par groupe (INSEE_COM)\n",
    "for col in numeric_cols:\n",
    "    # Calcul de la médiane par groupe\n",
    "    group_medians = df_sales_clean.groupby(GROUP_COL)[col].median()\n",
    "\n",
    "    # Remplacement des valeurs manquantes par la médiane du groupe, sinon par la médiane globale\n",
    "    group_values = df_sales_clean[GROUP_COL].map(group_medians)\n",
    "    global_median = df_sales_clean[col].median()\n",
    "    df_sales_clean[col] = df_sales_clean[col].fillna(group_values).fillna(global_median)\n",
    "\n",
    "# Colonnes traitées\n",
    "print(\"Colonnes numériques traitées :\")\n",
    "print(numeric_cols)\n",
    "print(\"\\nValeurs manquantes après imputation :\")\n",
    "print(df_sales_clean[numeric_cols].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca01069",
   "metadata": {},
   "source": [
    "## Standardisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff823a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialisation du scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# ---------- Étape 1 : Standardisation des variables numériques continues ----------\n",
    "\n",
    "\n",
    "df_sales_clean[numeric_cols] = scaler.fit_transform(df_sales_clean[numeric_cols])\n",
    "\n",
    "# ---------- Étape 2 : Standardisation des variables catégorielles ordinales ----------\n",
    "\n",
    "# Standardisation des colonnes ordinales\n",
    "df_sales_clean[ordinales] = scaler.fit_transform(df_sales_clean[ordinales])\n",
    "\n",
    "# ---------- Étape 3 : Standardisation des variables encodées avec TargetEncoder ----------\n",
    "\n",
    "# Standardisation des colonnes encodées avec TargetEncoder\n",
    "df_sales_clean[cat_cols_large] = scaler.fit_transform(df_sales_clean[cat_cols_large])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad67c7",
   "metadata": {},
   "source": [
    "### Vérifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbe9218b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes standardisées :\n",
      "    Numériques continues :\n",
      "         ['surface', 'surface_terrain', 'dpeC', 'places_parking', 'charges_copro', 'loyer_m2_median_n6', 'nb_log_n6', 'taux_rendement_n6', 'loyer_m2_median_n7', 'nb_log_n7', 'taux_rendement_n7', 'prix_m2_vente']\n",
      "    Ordinales :\n",
      "         ['ges_class', 'dpeL', 'logement_neuf', 'nb_pieces', 'bain', 'eau', 'nb_toilettes', 'balcon']\n",
      "    Encodées avec TargetEncoder :\n",
      "         ['etage', 'nb_etages', 'exposition', 'annee_construction', 'chauffage_energie', 'chauffage_systeme', 'date']\n",
      "\n",
      "\n",
      " Colonnes non standardisées après vérification :\n",
      "      ['cave', 'porte_digicode', 'ascenseur', 'INSEE_COM', 'typedebien_Maison/Villa neuve', 'typedebien_a', 'typedebien_an', 'typedebien_h', 'typedebien_l', 'typedebien_m', 'typedebien_mn', 'typedetransaction_pi', 'typedetransaction_v', 'typedetransaction_vp', 'chauffage_mode_Central', 'chauffage_mode_Collectif', 'chauffage_mode_Collectif, Central', 'chauffage_mode_Collectif, Individuel', 'chauffage_mode_Collectif, Individuel, Central', 'chauffage_mode_Individuel', 'chauffage_mode_Individuel, Central', 'chauffage_mode_MISSING', 'chauffage_energie_principal_Bois', 'chauffage_energie_principal_Fioul', 'chauffage_energie_principal_Gaz', 'chauffage_energie_principal_MISSING', 'chauffage_energie_principal_Électrique', 'x_geo', 'y_geo', 'z_geo']\n",
      "\n",
      "\n",
      "\n",
      "Colonnes restantes dans df_sales_clean après les encodages:\n",
      "\n",
      "      Index(['surface', 'surface_terrain', 'dpeC', 'places_parking', 'cave',\n",
      "       'porte_digicode', 'ascenseur', 'charges_copro', 'INSEE_COM',\n",
      "       'loyer_m2_median_n6', 'nb_log_n6', 'taux_rendement_n6',\n",
      "       'loyer_m2_median_n7', 'nb_log_n7', 'taux_rendement_n7', 'prix_m2_vente',\n",
      "       'ges_class', 'dpeL', 'logement_neuf', 'nb_pieces', 'bain', 'eau',\n",
      "       'nb_toilettes', 'balcon', 'etage', 'nb_etages', 'exposition',\n",
      "       'annee_construction', 'chauffage_energie', 'chauffage_systeme', 'date',\n",
      "       'typedebien_Maison/Villa neuve', 'typedebien_a', 'typedebien_an',\n",
      "       'typedebien_h', 'typedebien_l', 'typedebien_m', 'typedebien_mn',\n",
      "       'typedetransaction_pi', 'typedetransaction_v', 'typedetransaction_vp',\n",
      "       'chauffage_mode_Central', 'chauffage_mode_Collectif',\n",
      "       'chauffage_mode_Collectif, Central',\n",
      "       'chauffage_mode_Collectif, Individuel',\n",
      "       'chauffage_mode_Collectif, Individuel, Central',\n",
      "       'chauffage_mode_Individuel', 'chauffage_mode_Individuel, Central',\n",
      "       'chauffage_mode_MISSING', 'chauffage_energie_principal_Bois',\n",
      "       'chauffage_energie_principal_Fioul', 'chauffage_energie_principal_Gaz',\n",
      "       'chauffage_energie_principal_MISSING',\n",
      "       'chauffage_energie_principal_Électrique', 'x_geo', 'y_geo', 'z_geo'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vérifications\n",
    "print(\"Colonnes standardisées :\")\n",
    "print(\"    Numériques continues :\\n\",\"       \", numeric_cols)\n",
    "print(\"    Ordinales :\\n\", \"       \",ordinales)\n",
    "print(\"    Encodées avec TargetEncoder :\\n\", \"       \",cat_cols_large)\n",
    "\n",
    "# Liste des colonnes non standardisées par soustraction\n",
    "standardized_cols = numeric_cols + ordinales + cat_cols_large\n",
    "non_standardized_cols = [col for col in df_sales_clean.columns if col not in standardized_cols]\n",
    "\n",
    "# Affichage des colonnes non standardisées\n",
    "print(\"\\n\\n Colonnes non standardisées après vérification :\")\n",
    "print(\"     \", non_standardized_cols)\n",
    "print(\"\\n\\n\")\n",
    "# Verification des colonnes restantes avant le split\n",
    "print(\"Colonnes restantes dans df_sales_clean après les encodages:\\n\")\n",
    "print(\"     \",df_sales_clean.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3742e",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dce79511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de X_train : (3898764, 55)\n",
      "Shape de X_test : (974691, 55)\n",
      "Shape de y_train : (3898764,)\n",
      "Shape de y_test : (974691,)\n"
     ]
    }
   ],
   "source": [
    "#SPLIT \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_sales_clean.drop(columns=['prix_m2_vente', 'INSEE_COM'])\n",
    "y = df_sales_clean['prix_m2_vente']\n",
    "\n",
    "# Séparation du jeu de données en train et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Afficher les shapes des datasets\n",
    "print(\"Shape de X_train :\", X_train.shape)\n",
    "print(\"Shape de X_test :\", X_test.shape)\n",
    "print(\"Shape de y_train :\", y_train.shape)\n",
    "print(\"Shape de y_test :\", y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc799ac9",
   "metadata": {},
   "source": [
    "### Sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfc04418",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paths\n",
    "folder_path_M = '/Users/maximehenon/Documents/GitHub/MAR25_BDS_Compagnon_Immo/'\n",
    "# folder_path_Y = 'C:/Users/charl/OneDrive/Documents/Yasmine/DATASCIENTEST/FEV25-BDS-COMPAGNON'\n",
    "#folder_path_C = '../data/processed/Sales'\n",
    "#folder_path_L = '/Users/loick.d/Documents/Datascientest/Github immo/MAR25_BDS_Compagnon_Immo/'\n",
    "# folder_path_LW = 'C:/Users/User/Downloads/drive-download-20250508T155351Z-1-001'\n",
    "\n",
    "## stocker les datasets\n",
    "X_test.to_csv(os.path.join(folder_path_M, 'X_test_encoded.csv'), sep=';', index=False)\n",
    "X_train.to_csv(os.path.join(folder_path_M, 'X_train_encoded.csv'), sep=';', index=False)\n",
    "y_train.to_csv(os.path.join(folder_path_M, 'y_train_encoded.csv'), sep=';', index=False)\n",
    "y_test.to_csv(os.path.join(folder_path_M, 'y_test_encoded.csv'), sep=';', index=False)\n",
    "\n",
    "# X_test.to_csv(os.path.join(folder_path_Y, 'X_test_encoded.csv'), sep=';', index=False)\n",
    "# X_train.to_csv(os.path.join(folder_path_Y, 'X_train_encoded.csv'), sep=';', index=False)\n",
    "# y_train.to_csv(os.path.join(folder_path_Y, 'y_train_encoded.csv'), sep=';', index=False)\n",
    "# y_test.to_csv(os.path.join(folder_path_Y, 'y_test_encoded.csv'), sep=';', index=False)\n",
    "\n",
    "# X_test.to_csv(os.path.join(folder_path_C, 'X_test_encoded.csv'), sep=';', index=False)\n",
    "# X_train.to_csv(os.path.join(folder_path_C, 'X_train_encoded.csv'), sep=';', index=False)\n",
    "# y_train.to_csv(os.path.join(folder_path_C, 'y_train_encoded.csv'), sep=';', index=False)\n",
    "# y_test.to_csv(os.path.join(folder_path_C, 'y_test_encoded.csv'), sep=';', index=False)\n",
    "\n",
    "# X_test.to_Lsv(os.path.join(folder_path_L, 'X_test_encoded.csv'), sep=';', index=False)\n",
    "# X_train.to_Lsv(os.path.join(folder_path_L, 'X_train_encoded.csv'), sep=';', index=False)\n",
    "# y_train.to_Lsv(os.path.join(folder_path_L, 'y_train_encoded.csv'), sep=';', index=False)\n",
    "# y_test.to_Lsv(os.path.join(folder_path_L, 'y_test_encoded.csv'), sep=';', index=False)\n",
    "\n",
    "# X_test.to_csv(os.path.join(folder_path_LW, 'X_test_encoded.csv'), sep=';', index=False)\n",
    "# X_train.to_csv(os.path.join(folder_path_LW, 'X_train_encoded.csv'), sep=';', index=False)\n",
    "# y_train.to_csv(os.path.join(folder_path_LW, 'y_train_encoded.csv'), sep=';', index=False)\n",
    "# y_test.to_csv(os.path.join(folder_path_LW, 'y_test_encoded.csv'), sep=';', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
